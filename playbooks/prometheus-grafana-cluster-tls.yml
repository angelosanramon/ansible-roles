---
- name: generate certificates
  hosts: localhost
  become: yes
  roles:
    - name: letsencrypt
      vars:
        letsencrypt_acme_email: email@example.com
        letsencrypt_aws_access_key: aws_access_key
        letsencrypt_aws_secret_key: aws_secret_key
        letsencrypt_route53_zone: example.com
        letsencrypt_certificates:
          - common_name: example.com
            subject_alt_name:
              - prometheus.example.com
              - alertmanager.example.com
              - grafana.example.com
            valid_days: 7

    - name: selfsignedcerts
      vars:
        selfsignedcerts_server_certificates:
          - common_name: grafana1.cluster.local
            subject_alt_names:
              - IP:192.168.1.13
          - common_name: grafana2.cluster.local
            subject_alt_names:
              - IP:192.168.2.13

- name: setup drbd for prometheus cluster
  hosts: prometheus_servers
  become: yes
  vars:
    keepalived_vip: 192.168.3.101
  roles:
    - name: ntp
    - name: rsyslog
      vars:
        rsyslog_configurations:
          - name: drbd.conf
            configuration: |-
              if $programname == "drbd" or ($syslogtag == "kernel:" and
              $msg contains "drbd") then {
                  action(type="omfile" file="{{ drdb_log_path }}/{{ drbd_log_file }}")
                  stop
              }
    - name: firewalld
      vars:
        firewalld_ports:
          - "{{ drbd_listen_ports }}/tcp"
    - name: logrotate
      vars:
        logrotate_config_options:
          - name: drbd        
            paths:
              - "{{ drdb_log_path }}/*.log"
            options:
              - daily
              - missingok
              - rotate 7
              - dateext
              - copytruncate
              - |-
                postrotate
                    /usr/bin/systemctl reload drbd
                endscript
    - name: drbd
      vars:
        drbd_servers_group_name: prometheus_servers
        drbd_resources_conf:
          - name: drbd1
            meta-disk: internal
            device: /dev/drbd1
            disk: /dev/sdb1
            options:
              auto-promote: 'no'
            net:
              verify-alg: sha256
            'on':
              - name: prometheus1.cluster.local
                node-id: 0
                address: 192.168.3.210:7788
              - name: prometheus2.cluster.local
                node-id: 1
                address: 192.168.3.211:7788
            connection-mesh:
              hosts: prometheus1.cluster.local prometheus2.cluster.local
  post_tasks:
    - name: prepare resource device
      include_role:
        name: drbd
        tasks_from: prep_resource_device
      vars:
        mountpoint_path: /var/lib/prometheus
        mountpoint_owner: prometheus
        mountpoint_group: prometheus
        resource_device: /dev/drbd1
        resource_device_filesystem: xfs

    - name: create drbd before demote script
      copy:
        content: |
          /sbin/ip --brief add | /bin/grep -w {{ keepalived_vip }} > /dev/null
          if [ $? -eq 0 ]; then
              echo "Skipping demote. keepalived VIP is attached."
              exit 1
          fi

          # stop prometheus
          /bin/systemctl stop prometheus

          # wait for prometheus to stop
          loop_max=10
          loop_count=0
          while [ -n "`/sbin/pidof prometheus`" ] && 
          [ $loop_count -lt $loop_max ]; do
              echo "waiting for prometheus to stop"
              sleep 10
              loop_count=$((loop_count + 1))
          done

          if [ $loop_count -ge $loop_max ]; then
              echo "unable to stop prometheus"
              exit
          fi
        dest: "{{ drdb_script_path }}/pre_demote.sh"
        owner: root
        group: root
        mode: 0744

    - name: create drbd after demote script
      copy:
        content: |
          # start prometheus
          /bin/systemctl start prometheus

          # wait for prometheus to start
          loop_max=10
          loop_count=0
          stat_code=$(/bin/curl -o /dev/null -s -w "%{http_code}\n" \
          http://localhost:9090/status)
          while [ $stat_code -ne 200 ] && [ $loop_count -lt $loop_max ]; do
              echo "waiting for prometheus to start"
              stat_code=$(/bin/curl -o /dev/null -s -w "%{http_code}\n" \
              http://localhost:9090/status)
              loop_count=$((loop_count + 1))
              sleep 10
          done
        dest: "{{ drdb_script_path }}/post_demote.sh"
        owner: root
        group: root
        mode: 0744

    - name: create drbd before promote script
      copy:
        content: |
          /sbin/ip --brief add | /bin/grep -w {{ keepalived_vip }} > /dev/null
          if [ $? -ne 0 ]; then
              echo "Skipping promote. keepalived VIP is not attached."
              exit 1
          fi
        dest: "{{ drdb_script_path }}/pre_promote.sh"
        owner: root
        group: root
        mode: 0744

    - name: create drbd after successful promote script
      copy:
        content: |
          /bin/mount | /bin/grep -w drbd1
          if [ $? -eq 0 ]; then
              echo "prometheus volume already mounted"
          else
              echo "mounting prometheus volume"
              /bin/mount /dev/drbd1 /var/lib/prometheus
          fi

          echo "restarting prometheus server"
          /bin/systemctl restart prometheus

          # wait for prometheus to start
          loop_max=10
          loop_count=0
          stat_code=$(/bin/curl -o /dev/null -s -w "%{http_code}\n" \
          http://localhost:9090/status)
          while [ $stat_code -ne 200 ] && [ $loop_count -lt $loop_max ]; do
              echo "waiting for prometheus to start"
              stat_code=$(/bin/curl -o /dev/null -s -w "%{http_code}\n" \
              http://localhost:9090/status)
              loop_count=$((loop_count + 1))
              sleep 10
          done
        dest: "{{ drdb_script_path }}/post_promote.sh"
        owner: root
        group: root
        mode: 0744

    - name: create drbd after failed promote script
      copy:
        content: |
          /sbin/ip --brief add | /bin/grep -w {{ keepalived_vip }} > /dev/null
          if [ $? -eq 0 ]; then
              /bin/systemctl stop prometheus
          fi

          while true
          do
              /sbin/ip --brief add | /bin/grep -w {{ keepalived_vip }} > \
              /dev/null
              if [ $? -ne 0 ]; then
                  /bin/systemctl start prometheus
                  break
              fi

              sleep 5
          done
        dest: "{{ drdb_script_path }}/failed_promote.sh"
        owner: root
        group: root
        mode: 0744

- name: setup prometheus cluster
  hosts: prometheus_servers
  become: yes
  roles:
    - name: ntp
    - name: firewalld
      vars:
        firewalld_ports:
          - "{{ prometheus_web_listen_port }}/tcp"
          - "{{ alertmanager_web_listen_port }}/tcp"
          - "{{ alertmanager_cluster_port }}/tcp"
          - "{{ node_exporter_web_listen_port }}/tcp"
    - name: logrotate
      vars:
        logrotate_config_options:
          - name: prometheus        
            paths:
              - "{{ prometheus_log_path }}/*.log"
            options:
              - daily
              - missingok
              - rotate 7
              - dateext
              - copytruncate
              - |-
                postrotate
                    /usr/bin/systemctl reload prometheus
                endscript
          - name: alertmanager        
            paths:
              - "{{ alertmanager_log_path }}/*.log"
            options:
              - daily
              - missingok
              - rotate 7
              - dateext
              - copytruncate
              - |-
                postrotate
                    /usr/bin/systemctl reload alertmanager
                endscript
          - name: node_exporter        
            paths:
              - "{{ node_exporter_log_path }}/*.log"
            options:
              - daily
              - missingok
              - rotate 7
              - dateext
              - copytruncate
              - |-
                postrotate
                    /usr/bin/systemctl reload node_exporter
                endscript
    - name: prometheus
      include_vars: defaults/node_exporter.yml
      vars:
        prometheus_components:
          alertmanager:
            installed: true
          node_exporter:
            installed: true
        prometheus_scrape_configs:
          - job_name: prometheus
            static_configs:
              - targets: >-
                  {%- set servers = [] -%}
                  {%- for server in groups[prometheus_servers_group_name] -%}
                    {{- servers.append(server + ':' +
                    prometheus_web_listen_port|string) -}}
                  {%- endfor-%}
                  {{- servers -}}
          - job_name: node exporters
            static_configs:
              - targets: >-
                  {%- set servers = [] -%}
                  {%- for server in groups[prometheus_servers_group_name] -%}
                    {{- servers.append(server + ':' +
                    node_exporter_web_listen_port|string) -}}
                  {%- endfor-%}
                  {{- servers -}}
        prometheus_alertmanager_config:
          - static_configs:
            - targets: >-
                  {%- set servers = [] -%}
                  {%- for server in groups[prometheus_servers_group_name] -%}
                    {{- servers.append(server + ':' +
                    alertmanager_web_listen_port|string) -}}
                  {%- endfor-%}
                  {{- servers -}}
        alertmanager_servers_group_name: prometheus_servers
        alertmanager_route_config:
          group_by:
            - alertname
          group_wait: 10s
          group_interval: 10s
          repeat_interval: 1h
          receiver: web.hook
        alertmanager_receivers_config:
          - name: web.hook
            webhook_configs:
            - url: http://127.0.0.1:5001/

- name: setup keepalived for prometheus cluster
  hosts: prometheus_servers
  become: yes
  pre_tasks:
    - name: create keepalived script directory
      file:
        path: "{{ keepalived_scripts_path }}"
        state: directory

    - name: create prometheus check script
      copy:
        content: |-
          #!/bin/bash

          stat_code=$(/bin/curl -o /dev/null -s -w "%{http_code}\n" \
          http://localhost:9090/status)

          if [ $stat_code -eq 200 ]; then
              exit 0
          else
              exit 1
          fi
        dest: "{{ keepalived_config_path }}/scripts/chk_prometheus.sh"
        owner: root
        group: root
        mode: 0744
      register: prometheus_chk_script
  roles:
    - name: rsyslog
      vars:
        rsyslog_configurations:
          - name: keepalived.conf
            configuration: |-
              if ($programname startswith "Keepalived") then {
                  action(type="omfile" file="/var/log/keepalived/keepalived.log")
                  stop
              }
    - name: firewalld
      vars:
        firewalld_rich_rules:
          - rule protocol value='vrrp' accept
    - name: logrotate
      vars:
        logrotate_config_options:
          - name: keepalived
            paths:
              - /var/log/keepalived/*.log
            options:
              - daily
              - missingok
              - rotate 7
              - dateext
              - copytruncate
              - |-
                postrotate
                    /usr/bin/systemctl reload keepalived
                endscript
    - name: keepalived
      vars:
        keepalived_vip: 192.168.3.101
        keepalived_network: >-
          {{ keepalived_vip.split('.')[:-1]|join('.') + '.0' }}
        keepalived_configs:
          global_defs:
            script_user: root root
            enable_script_security:
            notification_email:
              - email@example.com
          vrrp_script:
            - name: chk_prometheus
              script: "{{ keepalived_config_path }}/scripts/chk_prometheus.sh"
              interval: 30
              weight: 2
          vrrp_instance:
            - name: PROMETHEUS_HA
              interface: |-
                {%- for key, value in ansible_facts.items() if
                  value.ipv4.network is defined and
                  value.ipv4.network == keepalived_network -%}
                {{- key -}}
                {%- endfor -%}
              state: >-
                {{- 'MASTER' if groups.prometheus_servers|
                first == inventory_hostname else 'BACKUP' -}}
              priority: >-
                {%- for hostname in groups.prometheus_servers|reverse -%}
                  {%- if hostname == inventory_hostname -%}
                    {{- loop.index -}}
                  {%- endif -%}
                {%- endfor -%}
              virtual_router_id: 1
              smtp_alert:
              authentication:
                auth_type: PASS
                auth_pass: mypassword
              unicast_src_ip: >-
                {%- for key, value in ansible_facts.items() if
                  value.ipv4.network is defined and
                  value.ipv4.network == keepalived_network -%}
                {{- value.ipv4.address -}}
                {%- endfor -%}
              unicast_peer: >-
                {%- set servers = [] -%}
                {%- for hostname in groups.prometheus_servers if
                  hostname != inventory_hostname -%}
                  {%- for key, value in hostvars[hostname].ansible_facts.items() if
                    value.ipv4.network is defined and
                    value.ipv4.network == keepalived_network -%}
                    {{- servers.append(value.ipv4.address) -}}
                  {%- endfor -%}
                {%- endfor -%}
                {{- servers -}}
              virtual_ipaddress:
                - "{{ keepalived_vip }}"
              track_script:
                - chk_prometheus
              notify: "{{ keepalived_scripts_path }}/promote_or_demote.sh"
  post_tasks:
    - name: reload keepalived
      service:
        name: keepalived
        state: reloaded
      when: prometheus_chk_script is changed

    - name: create cron job for demoting or promoting drbd resource
      copy:
        content: |
          #!/bin/bash

          if [ -n "`pidof -x $(basename $0) -o %PPID`" ]; then
              echo "$(basename $0) is already running."
              exit
          fi

          role=$(/sbin/drbdadm role all)

          /sbin/ip --brief add | /bin/grep -w {{ keepalived_vip }} > /dev/null
          if [ $? -eq 0 ] && [ "$role" == "Secondary" ]; then
              echo "Promoting to primary."
              /etc/drbd.d/scripts/promote_to_primary.sh
          fi

          /sbin/ip --brief add | /bin/grep -w {{ keepalived_vip }} > /dev/null
          if [ $? -ne 0 ] && [ "$role" == "Primary" ]; then
              echo "Demote to secondary."
              /etc/drbd.d/scripts/demote_to_secondary.sh
          fi
        dest: "{{ keepalived_scripts_path }}/promote_or_demote.sh"
        owner: root
        group: root
        mode: 0744

    - name: create cron job to promote or demote
      cron:
        name: "promote or demote drbd"
        minute: "*/1"
        job: "{{ keepalived_scripts_path }}/promote_or_demote.sh"

- name: setup drbd for grafana cluster
  hosts: grafana_servers
  become: yes
  vars:
    keepalived_vip: 192.168.3.102
  roles:
    - name: ntp
    - name: rsyslog
      vars:
        rsyslog_configurations:
          - name: drbd.conf
            configuration: |-
              if $programname == "drbd" or ($syslogtag == "kernel:" and
              $msg contains "drbd") then {
                  action(type="omfile" file="{{ drdb_log_path }}/{{ drbd_log_file }}")
                  stop
              }
    - name: firewalld
      vars:
        firewalld_ports:
          - "{{ drbd_listen_ports }}/tcp"
    - name: logrotate
      vars:
        logrotate_config_options:
          - name: drbd        
            paths:
              - "{{ drdb_log_path }}/*.log"
            options:
              - daily
              - missingok
              - rotate 7
              - dateext
              - copytruncate
              - |-
                postrotate
                    /usr/bin/systemctl reload drbd
                endscript
    - name: drbd
      vars:
        drbd_servers_group_name: grafana_servers
        drbd_resources_conf:
          - name: drbd1
            meta-disk: internal
            device: /dev/drbd1
            disk: /dev/sdb1
            options:
              auto-promote: 'no'
            net:
              verify-alg: sha256
            'on':
              - name: grafana1.cluster.local
                node-id: 0
                address: 192.168.3.208:7788
              - name: grafana2.cluster.local
                node-id: 1
                address: 192.168.3.209:7788
            connection-mesh:
              hosts: grafana1.cluster.local grafana2.cluster.local
  post_tasks:
    - name: prepare resource device
      include_role:
        name: drbd
        tasks_from: prep_resource_device
      vars:
        mountpoint_path: /var/lib/grafana
        mountpoint_owner: grafana
        mountpoint_group: grafana
        resource_device: /dev/drbd1
        resource_device_filesystem: xfs

    - name: create drbd before demote script
      copy:
        content: |
          /sbin/ip --brief add | /bin/grep -w {{ keepalived_vip }} > /dev/null
          if [ $? -eq 0 ]; then
              echo "Skipping demote. keepalived VIP is attached."
              exit 1
          fi

          # stop grafana
          /bin/systemctl stop grafana-server

          # wait for grafana to stop
          loop_max=10
          loop_count=0
          while [ -n "`/sbin/pidof grafana-server`" ] && 
          [ $loop_count -lt $loop_max ]; do
              echo "waiting for grafana-server to stop"
              loop_count=$((loop_count + 1))
              sleep 10
          done

          if [ $loop_count -ge $loop_max ]; then
              echo "unable to stop grafana"
              exit
          fi
        dest: "{{ drdb_script_path }}/pre_demote.sh"
        owner: root
        group: root
        mode: 0744

    - name: create drbd after demote script
      copy:
        content: |
          # start grafana
          /bin/systemctl start grafana-server

          # wait for grafana to start
          loop_max=10
          loop_count=0
          stat_code=$(/bin/curl -k -o /dev/null -s -w "%{http_code}\n" \
          https://localhost:3000/api/health)
          while [ $stat_code -ne 200 ] && [ $loop_count -lt $loop_max ]; do
              echo "waiting for grafana-server to start"
              stat_code=$(/bin/curl -k -o /dev/null -s -w "%{http_code}\n" \
              https://localhost:3000/api/health)
              loop_count=$((loop_count + 1))
              sleep 10
          done
        dest: "{{ drdb_script_path }}/post_demote.sh"
        owner: root
        group: root
        mode: 0744

    - name: create drbd before promote script
      copy:
        content: |
          /sbin/ip --brief add | /bin/grep -w {{ keepalived_vip }} > /dev/null
          if [ $? -ne 0 ]; then
              echo "Skipping promote. keepalived VIP is not attached."
              exit 1
          fi
        dest: "{{ drdb_script_path }}/pre_promote.sh"
        owner: root
        group: root
        mode: 0744

    - name: create drbd after successful promote script
      copy:
        content: |
          /bin/mount | /bin/grep -w drbd1
          if [ $? -eq 0 ]; then
              echo "grafana volume already mounted"
          else
              echo "mounting grafana volume"
              /bin/mount /dev/drbd1 /var/lib/grafana
          fi

          echo "restarting grafana server"
          /bin/systemctl restart grafana-server

          # wait for grafana to start
          loop_max=10
          loop_count=0
          stat_code=$(/bin/curl -k -o /dev/null -s -w "%{http_code}\n" \
          https://localhost:3000/api/health)
          while [ $stat_code -ne 200 ] && [ $loop_count -lt $loop_max ]; do
              echo "waiting for grafana-server to start"
              stat_code=$(/bin/curl -k -o /dev/null -s -w "%{http_code}\n" \
              https://localhost:3000/api/health)
              loop_count=$((loop_count + 1))
              sleep 10
          done
        dest: "{{ drdb_script_path }}/post_promote.sh"
        owner: root
        group: root
        mode: 0744

    - name: create drbd after failed promote script
      copy:
        content: |
          /sbin/ip --brief add | /bin/grep -w {{ keepalived_vip }} > /dev/null
          if [ $? -eq 0 ]; then
              /bin/systemctl stop grafana-server
          fi

          while true
          do
              /sbin/ip --brief add | /bin/grep -w {{ keepalived_vip }} > \
              /dev/null
              if [ $? -ne 0 ]; then
                  /bin/systemctl start grafana-server
                  break
              fi

              sleep 5
          done
        dest: "{{ drdb_script_path }}/failed_promote.sh"
        owner: root
        group: root
        mode: 0744

- name: setup grafana cluster
  hosts: grafana_servers
  become: yes
  pre_tasks:
    - name: copy letsencrypt certificates to grafana servers
      include_role:
        name: letsencrypt
        tasks_from: fetch_certificates.yml
      vars:
        cert_path: "{{ grafana_certs_path }}"
        cert_owner: "{{ grafana_user }}"
        cert_group: "{{ grafana_group }}"
        domain_name: example.com
      register: copy_letsencrypt_certs

    - name: copy self signed certificates
      include_role:
        name: selfsignedcerts
        tasks_from: fetch_certskeys.yml
      vars:
        cert_path: "{{ grafana_certs_path }}"
        cert_owner: "{{ grafana_user }}"
        cert_group: "{{ grafana_group }}"
      register: copy_selfsigned_certs
  roles:
    - name: ntp
    - name: rsyslog
      vars:
        rsyslog_configurations:
          - name: keepalived.conf
            configuration: |-
              if ($programname startswith "Keepalived") then {
                  action(type="omfile" file="/var/log/keepalived/keepalived.log")
                  stop
              }
    - name: firewalld
      vars:
        firewalld_ports:
          - "{{ grafana_listen_port }}/tcp"
    - name: logrotate
      vars:
        logrotate_config_options:
          - name: grafana        
            paths:
              - "{{ grafana_log_path }}/*.log"
            options:
              - daily
              - missingok
              - rotate 7
              - dateext
              - copytruncate
              - |-
                postrotate
                    /usr/bin/systemctl reload grafana
                endscript
    - name: grafana
      vars:
        grafana_configs:
          server:
            protocol: https
            cert_file: "{{ grafana_certs_path }}/{{ inventory_hostname }}.crt"
            cert_key: "{{ grafana_certs_path }}/{{ inventory_hostname }}.key"
          security:
            admin_user: grafana_admin
            admin_password: mypassword
  post_tasks:
    - name: restart grafana
      service:
        name: grafana-server
        state: restarted
      when: copy_letsencrypt_certs is changed or
            copy_selfsigned_certs is changed

- name: setup keepalived for grafana cluster
  hosts: grafana_servers
  become: yes
  pre_tasks:
    - name: create keepalived script directory
      file:
        path: "{{ keepalived_scripts_path }}"
        state: directory

    - name: create grafana check script
      copy:
        content: |-
          #!/bin/bash

          stat_code=$(/bin/curl -k -o /dev/null -s -w "%{http_code}\n" \
          https://localhost:3000/api/health)

          if [ $stat_code -eq 200 ]; then
              exit 0
          else
              exit 1
          fi
        dest: "{{ keepalived_config_path }}/scripts/chk_grafana.sh"
        owner: root
        group: root
        mode: 0744
      register: grafana_chk_script
  roles:
    - name: rsyslog
      vars:
        rsyslog_configurations:
          - name: keepalived.conf
            configuration: |-
              if ($programname startswith "Keepalived") then {
                  action(type="omfile" file="/var/log/keepalived/keepalived.log")
                  stop
              }
    - name: firewalld
      vars:
        firewalld_rich_rules:
          - rule protocol value='vrrp' accept
    - name: logrotate
      vars:
        logrotate_config_options:
          - name: keepalived
            paths:
              - /var/log/keepalived/*.log
            options:
              - daily
              - missingok
              - rotate 7
              - dateext
              - copytruncate
              - |-
                postrotate
                    /usr/bin/systemctl reload keepalived
                endscript
    - name: keepalived
      vars:
        keepalived_vip: 192.168.3.102
        keepalived_network: >-
          {{ keepalived_vip.split('.')[:-1]|join('.') + '.0' }}
        keepalived_configs:
          global_defs:
            script_user: root root
            enable_script_security:
            notification_email:
              - email@example.com
          vrrp_script:
            - name: chk_grafana
              script: "{{ keepalived_config_path }}/scripts/chk_grafana.sh"
              interval: 30
              weight: 2
          vrrp_instance:
            - name: GRAFANA_HA
              interface: |-
                {%- for key, value in ansible_facts.items() if
                  value.ipv4.network is defined and
                  value.ipv4.network == keepalived_network -%}
                {{- key -}}
                {%- endfor -%}
              state: >-
                {{- 'MASTER' if groups.grafana_servers|
                first == inventory_hostname else 'BACKUP' -}}
              priority: >-
                {%- for hostname in groups.grafana_servers|reverse -%}
                  {%- if hostname == inventory_hostname -%}
                    {{- loop.index -}}
                  {%- endif -%}
                {%- endfor -%}
              virtual_router_id: 2
              smtp_alert:
              authentication:
                auth_type: PASS
                auth_pass: mypassword
              unicast_src_ip: >-
                {%- for key, value in ansible_facts.items() if
                  value.ipv4.network is defined and
                  value.ipv4.network == keepalived_network -%}
                {{- value.ipv4.address -}}
                {%- endfor -%}
              unicast_peer: >-
                {%- set servers = [] -%}
                {%- for hostname in groups.grafana_servers if
                  hostname != inventory_hostname -%}
                  {%- for key, value in hostvars[hostname].ansible_facts.items() if
                    value.ipv4.network is defined and
                    value.ipv4.network == keepalived_network -%}
                    {{- servers.append(value.ipv4.address) -}}
                  {%- endfor -%}
                {%- endfor -%}
                {{- servers -}}
              virtual_ipaddress:
                - "{{ keepalived_vip }}"
              track_script:
                - chk_grafana
              notify: "{{ keepalived_scripts_path }}/promote_or_demote.sh"
  post_tasks:
    - name: reload keepalived
      service:
        name: keepalived
        state: reloaded
      when: grafana_chk_script is changed

    - name: create cron job for demoting or promoting drbd resource
      copy:
        content: |
          #!/bin/bash

          if [ -n "`pidof -x $(basename $0) -o %PPID`" ]; then
              echo "$(basename $0) is already running."
              exit
          fi

          role=$(/sbin/drbdadm role all)

          /sbin/ip --brief add | /bin/grep -w {{ keepalived_vip }} > /dev/null
          if [ $? -eq 0 ] && [ "$role" == "Secondary" ]; then
              echo "Promoting to primary."
              /etc/drbd.d/scripts/promote_to_primary.sh
          fi

          /sbin/ip --brief add | /bin/grep -w {{ keepalived_vip }} > /dev/null
          if [ $? -ne 0 ] && [ "$role" == "Primary" ]; then
              echo "Demote to secondary."
              /etc/drbd.d/scripts/demote_to_secondary.sh
          fi
        dest: "{{ keepalived_scripts_path }}/promote_or_demote.sh"
        owner: root
        group: root
        mode: 0744

    - name: create cron job to promote or demote
      cron:
        name: "promote or demote drbd"
        minute: "*/1"
        job: "{{ keepalived_scripts_path }}/promote_or_demote.sh"

- name: setup haproxy for prometheus and grafana cluster
  hosts: haproxy_servers
  become: yes
  pre_tasks:
    - name: copy letsencrypt certificates
      include_role:
        name: letsencrypt
        tasks_from: fetch_certificates.yml
      vars:
        cert_path: "{{ haproxy_certs_path }}"
        cert_owner: "{{ haproxy_user }}"
        cert_group: "{{ haproxy_group }}"
        domain_name: example.com
      register: copy_letsencrypt_certs

    - name: copy self signed certificates
      include_role:
        name: selfsignedcerts
        tasks_from: fetch_ca_certificate.yml
      vars:
        cert_path: "{{ haproxy_certs_path }}"
        cert_owner: "{{ haproxy_user }}"
        cert_group: "{{ haproxy_group }}"
      register: copy_selfsigned_certs
  roles:
    - name: ntp
    - name: rsyslog
      vars:
        rsyslog_configurations:
          - name: keepalived.conf
            configuration: |-
              if ($programname startswith "Keepalived") then {
                  action(type="omfile" file="/var/log/keepalived/keepalived.log")
                  stop
              }
    - name: firewalld
      vars:
        firewalld_ports:
          - 8404/tcp
          - 9090/tcp
          - 9093/tcp
          - 3000/tcp
        firewalld_rich_rules:
          - rule protocol value='vrrp' accept
    - name: logrotate
      vars:
        logrotate_config_options:
          - name: haproxy
            paths:
              - /var/log/haproxy/*.log
            options:
              - daily
              - missingok
              - rotate 7
              - dateext
              - copytruncate
              - |-
                postrotate
                    /usr/bin/systemctl reload haproxy
                endscript
          - name: keepalived
            paths:
              - /var/log/keepalived/*.log
            options:
              - daily
              - missingok
              - rotate 7
              - dateext
              - copytruncate
              - |-
                postrotate
                    /usr/bin/systemctl reload keepalived
                endscript
    - name: keepalived
      vars:
        keepalived_vip: 192.168.3.100
        keepalived_network: >-
          {{ keepalived_vip.split('.')[:-1]|join('.') + '.0' }}
        keepalived_configs:
          global_defs:
            script_user: root root
            enable_script_security:
            notification_email:
              - email@example.com
          vrrp_script:
            - name: chk_haproxy
              script: "'/usr/sbin/pidof haproxy'"
              interval: 30
              weight: 2
          vrrp_instance:
            - name: HAPROXY_HA
              interface: |-
                {%- for key, value in ansible_facts.items() if
                  value.ipv4.network is defined and
                  value.ipv4.network == keepalived_network -%}
                {{- key -}}
                {%- endfor -%}
              state: >-
                {{- 'MASTER' if groups.haproxy_servers|first == 
                inventory_hostname else 'BACKUP' -}}
              priority: >-
                {%- for hostname in groups.haproxy_servers|reverse -%}
                  {%- if hostname == inventory_hostname -%}
                    {{- loop.index -}}
                  {%- endif -%}
                {%- endfor -%}
              virtual_router_id: 3
              smtp_alert:
              authentication:
                auth_type: PASS
                auth_pass: mypassword
              unicast_src_ip: >-
                {%- for key, value in ansible_facts.items() if
                  value.ipv4.network is defined and
                  value.ipv4.network == keepalived_network -%}
                {{- value.ipv4.address -}}
                {%- endfor -%}
              unicast_peer: >-
                {%- set servers = [] -%}
                {%- for hostname in groups.haproxy_servers if
                  hostname != inventory_hostname -%}
                  {%- for key, value in hostvars[hostname].ansible_facts.items() if
                    value.ipv4.network is defined and
                    value.ipv4.network == keepalived_network -%}
                    {{- servers.append(value.ipv4.address) -}}
                  {%- endfor -%}
                {%- endfor -%}
                {{- servers -}}
              virtual_ipaddress:
                - "{{ keepalived_vip }}"
              track_script:
                - chk_haproxy
    - name: haproxy
      vars:
        haproxy_defaults_config:
          timeout:
            - connect 10s
            - client 30s
            - server 30s
          log: global
          mode: http
          option: httplog
        haproxy_frontends_config:
          prometheus_cluster:
            bind: 192.168.3.100:9090 ssl crt {{ 
                  haproxy_certs_path }}/example.com.pem
            acl: >-
              header_host_hostname hdr(host) -i -m beg prometheus.example.com
            use_backend: prometheus_servers if header_host_hostname
          alertmanager_cluster:
            bind: 192.168.3.100:9093 ssl crt {{ 
                  haproxy_certs_path }}/example.com.pem
            acl: >-
              header_host_hostname hdr(host) -i -m beg alertmanager.example.com
            use_backend: alertmanager_servers if header_host_hostname
          grafana_cluster:
            bind: 192.168.3.100:3000 ssl crt {{ 
                  haproxy_certs_path }}/example.com.pem
            acl: >-
              header_host_hostname hdr(host) -i -m beg grafana.example.com
            use_backend: grafana_servers if header_host_hostname
        haproxy_backends_config:
          prometheus_servers:
            balance: first
            option: httpchk GET /status
            default-server: check inter 10s
            server: prometheus 192.168.3.101:9090
          alertmanager_servers:
            balance: first
            option: httpchk GET /#/status
            default-server: check inter 10s
            server: alertmanager 192.168.3.101:9093
          grafana_servers:
            balance: first
            option: httpchk GET /api/health
            default-server: >-
              check inter 10s ssl ca-file {{ haproxy_certs_path }}/ca.crt
            server: grafana 192.168.3.102:3000
        haproxy_firewall_ports:
          - 9090/tcp
          - 9093/tcp
          - 3000/tcp
          - 8404/tcp
  post_tasks:
    - name: install node exporter
      include_role:
        name: prometheus
        tasks_from: node_exporter_install.yml
        defaults_from: main/node_exporter.yml
      vars:
        node_exporter_cmdline_options:
          - --collector.systemd

    - name: install haproxy exporter
      include_role:
        name: prometheus
        tasks_from: haproxy_exporter_install.yml
        defaults_from: main/haproxy_exporter.yml

    - name: reload haproxy
      service:
        name: haproxy
        state: reloaded
      when: copy_letsencrypt_certs is changed or
            copy_selfsigned_certs is changed
